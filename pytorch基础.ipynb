{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd1:\n",
      " type:  <class 'numpy.ndarray'>\n",
      "value:\n",
      " [[[4.22793572e-307 1.69122046e-306]\n",
      "  [1.05700260e-307 1.44635488e-307]\n",
      "  [3.33773471e-307 4.22793572e-307]]]\n",
      "\n",
      "\n",
      "nd2:\n",
      " type:  <class 'numpy.ndarray'>\n",
      "value:\n",
      " [[2 4 1 4]\n",
      " [4 3 2 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list1 = [1, 3, 2]\n",
    "\n",
    "# 从已有数据类型list生成。list中的数据指的是ndarray的维度大小。\n",
    "nd1 = np.ndarray(list1)\n",
    "print(\"nd1:\\n\", \"type: \", type(nd1))\n",
    "print(\"value:\\n\", nd1)\n",
    "\n",
    "list2 = [[2, 4, 1, 4], [4, 3, 2, 1]]\n",
    "nd2 = np.array(list2)  # 嵌套数据可以转换为多维ndarray。\n",
    "print(\"\\n\")\n",
    "print(\"nd2:\\n\", \"type: \", type(nd2))\n",
    "print(\"value:\\n\", nd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand1:\n",
      " [[0.54159249 0.5121131 ]\n",
      " [0.77661695 0.67546711]]\n",
      "rand2:\n",
      " [[6.57377192 8.18141366]\n",
      " [9.87104655 6.30292479]]\n",
      "rand3:\n",
      " [[ 0.17964116  0.12945487]\n",
      " [-3.05509823 -0.73564728]]\n"
     ]
    }
   ],
   "source": [
    "# numpy.random的使用\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# np.random.seed(1)  # 设置随机数种子，使每次生成的随机数一样。\n",
    "rand1 = np.random.random([2, 2])  # 生成0到1之间的随机数。\n",
    "rand2 = np.random.uniform(0, 10, [2, 2])  # 生成均匀分布的随机数。\n",
    "rand3 = np.random.randn(2, 2)  # 标准正态的随机数\n",
    "np.random.shuffle(rand3)  # 随机原地打乱顺序\n",
    "\n",
    "print(\"rand1:\\n\", rand1)\n",
    "print(\"rand2:\\n\", rand2)\n",
    "print(\"rand3:\\n\", rand3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np1:\n",
      " [[0 0 0]\n",
      " [0 0 0]]\n",
      "np2:\n",
      " [[1 1 1]\n",
      " [1 1 1]]\n",
      "np3:\n",
      " [[1 0]\n",
      " [0 1]]\n",
      "np4:\n",
      " [[4.22793572e-307 1.69122046e-306 1.05700260e-307]\n",
      " [1.44635488e-307 3.33773471e-307 4.22793572e-307]]\n",
      "np5:\n",
      " [[6 6 6]\n",
      " [6 6 6]]\n",
      "ar1:\n",
      " [10  9  8  7  6  5  4  3  2  1]\n",
      "lin1:\n",
      " [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# 创建特定形状的多维数组\n",
    "import numpy as np\n",
    "\n",
    "np1 = np.zeros(shape=(2, 3), dtype=np.uint8)  # 大小为2x2的元素全为0的数组\n",
    "np2 = np.ones(shape=(2, 3), dtype=np.uint8)  # 大小为2x2的元素全为1的数组\n",
    "\n",
    "# 大小为2x2，左上到右下对角线元素为1，其余元素为0的矩阵.\n",
    "np3 = np.eye(2, dtype=np.uint8)\n",
    "np4 = np.empty(shape=(2, 3))  # 大小为2x2的空数组，其数据的值是未初始化的垃圾值。\n",
    "np5 = np.full((2, 3), 6)  # 大小为2x3，且全部元素都为6的数组。\n",
    "\n",
    "# 利用arange、linspace函数生成数组\n",
    "ar1 = np.arange(10, 0, -1)  # 参数依次为初始值、终止值、步长（默认为1）\n",
    "lin1 = np.linspace(0, 1, 5)  # 将初始值到终止值分为参数num份，\n",
    "\n",
    "print(\"np1:\\n\", np1), print(\"np2:\\n\", np2)\n",
    "print(\"np3:\\n\", np3), print(\"np4:\\n\", np4)\n",
    "print(\"np5:\\n\", np5), print(\"ar1:\\n\", ar1),\n",
    "print(\"lin1:\\n\", lin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi2:\n",
      " [[6 6]\n",
      " [6 6]]\n",
      "dot:\n",
      " [[10 12 20]\n",
      " [ 8 10 16]]\n"
     ]
    }
   ],
   "source": [
    "# numpy的算数运算\n",
    "import numpy as np\n",
    "\n",
    "# 逐元素相乘，函数np.multiply(),或*。\n",
    "np1 = np.ones((2, 2), dtype=np.uint8) * 3\n",
    "np2 = np.ones((2, 2), dtype=np.uint8) * 2\n",
    "multi1 = np.multiply(np1, np2)  # 相乘\n",
    "multi2 = np1 * np2  # 相乘\n",
    "multi3 = np1 * 2  # 直接和标量相乘，每个元素位置都乘以标量。\n",
    "\n",
    "# 点积运算，遵循矩阵乘法\n",
    "# numpy.dot(a, b, out=None)\n",
    "a = np.array([[2, 3], [2, 2]])\n",
    "b = np.array([[2, 3, 4], [2, 2, 4]])\n",
    "dot = np.dot(a, b)\n",
    "\n",
    "print(\"multi2:\\n\", multi2)\n",
    "print(\"dot:\\n\", dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数组变形。包括展平、连接等。\n",
    "import numpy as np\n",
    "\n",
    "# reshape更改维度，不修改向量本身。\n",
    "arr1 = np.random.randint(0, 10, size=(2, 2))  # 2x2数组\n",
    "arr1.reshape(1, 4)  # reshape函数将向量维度更改为1行4列，但是arr1仍然是2x2大小。\n",
    "\n",
    "# resize更改维度，且修改向量本身。\n",
    "arr2 = np.random.randint(0, 10, size=(1, 2))\n",
    "\n",
    "# resize函数将维度改为2行1列，且重新赋值给arr2，相当于arr2=arr2.resize()。\n",
    "arr2.resize(2, 1)\n",
    "arr3 = (np.array([[1, 2, 3]])).T  # 转置\n",
    "\n",
    "# 展平操作，包括ravel和flatten，ravel不会产生原数组的副本，flatten返回原数组的副本。\n",
    "re = np.array([[1, 2], [3, 4]])\n",
    "reval = re.ravel()  # 值为[1,2,3,4]\n",
    "flat = np.array([[3, 2], [2, 4]])\n",
    "flatten = flat.flatten()  # 值为[3,2,2,4]\n",
    "\n",
    "# squeeze把矩阵中大小为1的维度去掉，unsqueeze则是在指定位置增加大小为1的维度。\n",
    "sq = np.ones((3, 2, 1))\n",
    "# print(sq.squeeze()) #去掉大小为1的维度\n",
    "# print(sq.squeeze().shape) #结果为（3,2）\n",
    "\n",
    "# transpose，对高维矩阵进行轴对换，比如把图片中表示颜色顺序的RGB改为GBR。\n",
    "trans = np.arange(24).reshape(2, 3, 4)\n",
    "# print(trans.shape)  # (2, 3, 4)\n",
    "# print(trans.transpose(1, 2, 0).shape)  # 调换0，1,2通道的顺序，结果是(3, 4, 2)\n",
    "\n",
    "# 合并数组。包括append、concatenate、stack、hstack、vstack、dstack、vsplit等。\n",
    "app1 = np.arange(4).reshape(2, 2)\n",
    "app2 = np.arange(4).reshape(2, 2)\n",
    "lian_jie1 = np.append(app1, app2, axis=0)  # 按行合并，大小是4x2。\n",
    "lian_jie2 = np.append(app1, app2, axis=1)  # 按列合并，大小是2x4。\n",
    "\n",
    "app11 = np.array([[1, 2], [3, 4]])\n",
    "app12 = np.array([[2, 3]])\n",
    "\n",
    "# 对哪个维度使用concatenate，需要确保另一个维度的大小相等才能成功。\n",
    "concatenate1 = np.concatenate((app11, app12), axis=0)  # 按行连接，大小是3x2。\n",
    "\n",
    "# 按列连接，结果是[[1,2,2],[3,4,3]]\n",
    "concatenate2 = np.concatenate((app11, app12.T), axis=1)\n",
    "# stack是沿指定轴堆叠数组或矩阵。注意只是堆叠效果，并没有合并。\n",
    "sta1 = np.array([[2, 3], [5, 4]])\n",
    "sta2 = np.array([[7, 6], [9, 1]])\n",
    "stack = np.stack((sta1, sta2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 torch基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# torch.tensor和torch.Tensor区别：\n",
    "# 1）torch.Tensor是torch.empty和torch.tensor之间的一种混合，\n",
    "# 但是，当传入数据时，torch.Tensor使用全局默认dtype（FloatTensor），\n",
    "# 而torch.tensor是从数据中推断数据类型。\n",
    "\n",
    "# 2）torch.tensor(1)返回一个固定值1，\n",
    "# 而torch.Tensor(1)返回一个大小为1的张量，它是随机初始化的值。\n",
    "\n",
    "# 常用张量创建方法\n",
    "t1 = torch.tensor(1)  # 结果是固定值1，t1类型是torch.LongTensor\n",
    "t2 = torch.Tensor(1)  # 结果是大小为1的张量，t2类型是torch.FloatTensor\n",
    "torch_eye = torch.eye(2, 2)  # 单位矩阵\n",
    "torch_zero = torch.zeros(2, 3)  # 0矩阵\n",
    "torch_one = torch.ones(3, 2)  # 1矩阵\n",
    "linspace = torch.linspace(0, 10, 3)  # 0~10分三份，结果是0、5、10。\n",
    "rand = torch.rand(2, 3)  # 均匀分布随机数\n",
    "randn = torch.randn(3, 2)  # 标准分布随机数\n",
    "\n",
    "# 修改张量形状\n",
    "torch_size = torch.tensor([[1, 2], [3, 4]])\n",
    "# print(torch_size.size())  # size和shape等价，返回结果都是torch.Size([2, 2])\n",
    "# print(torch_size.shape)\n",
    "# print(torch_size.view(4, 1))  # torch.view将其变为1x4矩阵。\n",
    "# print(torch_size.view(-1))  # 参数为-1时表示展平数组。\n",
    "# print(torch_size.unsqueeze(0))  # 增加一个维度\n",
    "# print(torch_size.numel())  # 返回矩阵的元素数量。\n",
    "\n",
    "# torch.view和torch.reshape()的区别：\n",
    "# 1）reshape()可以由torch.reshape()，也可由torch.Tensor.reshape()调用。但view()只可由torch.Tensor.view()来调用。\n",
    "# 2）对于一个将要被view的Tensor，新的size必须与原来的size与stride兼容。否则，在view之前必须调用contiguous()方法。\n",
    "# 3）同样也是返回与input数据量相同，但形状不同的Tensor。若满足view的条件，则不会copy，若不满足，则会copy。\n",
    "# 4）如果只想重塑张量，请使用torch.reshape。如果还关注内存使用情况并希望确保两个张量共享相同的数据，使用torch.view。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 python列表到pytorch张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(3)\n",
    "s = torch.tensor([[1, 2], [4, 21], [2, 4]], dtype=torch.float32)\n",
    "# print(s.shape) # torch.Size([3, 2])\n",
    "some_list = list(range(6))\n",
    "# print(some_list[:])\n",
    "# print(some_list[0:4])  # 第0个元素到第三个元素，不包含第四个元素。\n",
    "# print(some_list[1:])  # 第一个元素及之后的元素\n",
    "# print(some_list[:4])  # 第四个元素之前的元素，不包含第四个元素。\n",
    "# print(some_list[:-1])  # 最后一个元素之前的元素\n",
    "# print(some_list[1:5:2])  # 第一个元素到第五个元素，步长2，不包含第五个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "tensor([[[[ 1.3317, -0.3455,  0.3835, -2.3680, -1.8817],\n",
      "          [-0.0970, -0.4404, -1.0819,  0.0136,  0.0991],\n",
      "          [ 0.1068, -0.3804,  0.6227, -1.1191, -0.5852],\n",
      "          [ 1.4787, -1.4190, -0.7759,  1.0062,  0.0656],\n",
      "          [-0.3153,  0.9107, -0.7760, -0.5247, -0.1659]],\n",
      "\n",
      "         [[-0.0888, -0.9776, -1.0956, -0.6463,  0.0157],\n",
      "          [-1.6564, -1.2394,  0.8805, -0.9841,  0.6795],\n",
      "          [-1.6836, -0.9358,  0.2225,  0.9888, -2.2063],\n",
      "          [ 0.0315, -0.5758,  0.9893,  1.8348,  0.4028],\n",
      "          [ 1.4988, -0.3120, -0.5384,  0.9154, -0.3219]],\n",
      "\n",
      "         [[-1.2571,  1.1100,  0.4570,  0.2759, -0.3344],\n",
      "          [-0.4061, -0.7592, -0.7412,  1.0445, -0.3408],\n",
      "          [-0.4523, -1.0053,  0.4949, -1.5809, -0.3259],\n",
      "          [-0.3657,  0.7741,  1.1448,  0.8759, -0.4817],\n",
      "          [ 0.7621,  0.2310, -0.5415,  0.9914,  1.2362]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1369,  0.0939, -0.5127,  0.5519,  0.1398],\n",
      "          [-0.2210,  0.9679,  2.5564, -0.7045, -1.3222],\n",
      "          [-0.4315, -0.6507,  0.0867,  0.7962, -1.1479],\n",
      "          [ 1.6821, -0.7010,  0.6067,  0.1968,  0.4511],\n",
      "          [-0.5039,  0.1154, -0.8367,  1.4813, -0.4646]],\n",
      "\n",
      "         [[-0.4011, -0.5850,  0.7223,  1.4265,  0.8908],\n",
      "          [-1.1982, -0.0228, -0.2040,  0.0632, -1.3316],\n",
      "          [-0.0986,  0.2826, -1.5761, -0.0994, -0.3125],\n",
      "          [ 0.6455,  1.2554,  0.2027,  2.4409,  0.7386],\n",
      "          [ 0.3228,  0.0101, -0.4785, -0.0595,  0.1984]],\n",
      "\n",
      "         [[ 0.9388,  0.7114, -1.9419, -0.1988, -0.6190],\n",
      "          [-1.4679, -1.5956,  0.7071,  0.7960, -0.7546],\n",
      "          [-1.3931,  0.0604, -0.0977,  0.8623,  1.0603],\n",
      "          [ 0.2954,  0.6629,  0.2610, -0.7158, -0.3431],\n",
      "          [-0.2154,  1.0181, -0.1722, -1.3780,  0.7190]]]],\n",
      "       names=('batch', 'channel', 'row', 'column'))\n"
     ]
    }
   ],
   "source": [
    "batch = torch.randn(2, 3, 5, 5)\n",
    "print(batch.mean(-3).shape)\n",
    "# refine_names可以重新命名张量每个通道的名字，省略号表示靠前面的通道省略不更改。\n",
    "batch_named = batch.refine_names(..., \"batch\", \"channel\", \"row\", \"column\")\n",
    "print(batch_named)  # 返回的仍然是那个张量，只是通道名称改变了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定数据类型的方法\n",
    "double_tensor1 = torch.ones(2, 3, dtype=torch.double)  # double就是float64\n",
    "double_tensor2 = torch.ones(2, 3).double()  # 点方法\n",
    "\n",
    "# 使用to，底层会检查转换是否必要。\n",
    "# 在操作中输入多种类型时，输入会自动向较大类型转换。\n",
    "double_tensor3 = torch.ones(2, 3).to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.0\n",
      " 1.0\n",
      " 1.0\n",
      " 1.0\n",
      " 1.0\n",
      " 1.0\n",
      "[torch.storage.TypedStorage(dtype=torch.float64, device=cpu) of size 6]\n",
      "tensor([[2., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 转置\n",
    "\n",
    "double_tensor1 = torch.ones(2, 3, dtype=torch.double)  # double就是float64\n",
    "transpose1 = torch.transpose(double_tensor1, 0, 1)  # 转置函数\n",
    "transpose2 = double_tensor1.transpose(0, 1)  # 转置方法\n",
    "\n",
    "print(transpose1.storage())  # 访问张量的存储区，\n",
    "# 尽管张量显示自己有3行2列，但底层的存储区是一个大小为6的连续数组。\n",
    "# 从这个意义上说，张量只知道如何将一对索引转换成存储区中的一个位置。\n",
    "\n",
    "# 如果改变存储区的值，原值就会跟着改变。\n",
    "transpose1_storage = transpose1.storage()\n",
    "transpose1_storage[0] = 2  # 将存储区的第一个数改变，原值就会改变。\n",
    "\n",
    "print(transpose1)  # 第一个数已经改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "one stride:  (2, 1)\n",
      "tensor([[11,  3],\n",
      "        [ 3,  4],\n",
      "        [ 5,  4]])\n"
     ]
    }
   ],
   "source": [
    "# 原地修改，一般在方法的末尾加下划线表示。意思就是修改之后将值返回给原来的变量。\n",
    "one = torch.ones(2, 2)\n",
    "one.zero_()\n",
    "\n",
    "print(\"one:\\n\", one)  # 已经变为0\n",
    "\n",
    "# 步长是一个元组，指示当索引在每个维度中增加1时在存储区中必须跳过的元素数量。\n",
    "print(\"one stride: \", one.stride())  # 结果是（2,1）\n",
    "\n",
    "# 更改子张量同样会导致原张量更改，因为都是对同样的存储空间进行操作。\n",
    "large_tensor = torch.tensor([[2, 3], [3, 4], [5, 4]])\n",
    "small_tensor = large_tensor[0]  # 将第0行,即[2,3]作为子张量\n",
    "small_tensor[0] = 11  # 修改子张量的第0个数，即将0改为11。\n",
    "print(large_tensor)  # 原张量变为[[11,3],[3,4],[5,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5, 4],\n",
      "        [2, 7, 9]])\n",
      "转置后是否共享存储空间：  True\n",
      "转置前后步长对比：  (2, 1) (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# 转置，转置之后也是和原张量使用同样底层存储空间。不分配新内存，只创建新的tensor实例。\n",
    "\n",
    "tensor = torch.tensor([[3, 2], [5, 7], [4, 9]])\n",
    "t_tensor = tensor.t()  # 转置方法.t()\n",
    "print(t_tensor)  # [[3,5,4],[2,7,9]]\n",
    "\n",
    "# 验证是否共享存储区，返回True。\n",
    "print(\"转置后是否共享存储空间： \", id(tensor.storage()) == id(t_tensor.storage()))\n",
    "print(\"转置前后步长对比： \", tensor.stride(), t_tensor.stride())  # 步长对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 2])\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "# 任意维度的转置\n",
    "some_tensor = torch.ones(2, 3, 5)\n",
    "t_some_tensor = some_tensor.transpose(0, 2)  # 交换第0维和第2维\n",
    "print(t_some_tensor.shape)  # 形状变为torch.Size([5, 3, 2])\n",
    "\n",
    "# 转置之后存储空间不连续。\n",
    "print(some_tensor.is_contiguous(), t_some_tensor.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.0\n",
      " 3.0\n",
      " 5.0\n",
      " 7.0\n",
      " 4.0\n",
      " 9.0\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]\n",
      " 2.0\n",
      " 5.0\n",
      " 4.0\n",
      " 3.0\n",
      " 7.0\n",
      " 9.0\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]\n"
     ]
    }
   ],
   "source": [
    "# 使用contiguous()方法将转置之后的张量存储空间变为连续。\n",
    "points = torch.tensor([[2.0, 3.0], [5.0, 7.0], [4.0, 9.0]])\n",
    "t_points = points.t()\n",
    "\n",
    "# 转置后是[[2., 5., 4.],[3., 7., 9.]]，\n",
    "# 但存储顺序是按照列排列的2,3,5,7,4,9，不是行排列。\n",
    "print(t_points.storage())\n",
    "contiguous_points = t_points.contiguous()  # 转换为连续存储空间\n",
    "\n",
    "# 连续存储空间是按照行排列，结果是2., 5., 4.,3., 7., 9.\n",
    "print(contiguous_points.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量存储到GPU，加速运算。\n",
    "gpu_tensor = torch.tensor(\n",
    "    [[2, 3], [5, 4]], device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# 直接将创建的张量复制到gpu。\n",
    "gpu_tensor1 = gpu_tensor.to(device=\"cuda:0\")\n",
    "\n",
    "# 运算是在gpu上执行，gpu分配一个新的张量存储该结果的句柄，没有信息流到cpu。\n",
    "gpu_tensor2 = gpu_tensor + 3\n",
    "\n",
    "# 使用.to()方法将张量转移到cpu。\n",
    "gpu_tensor2 = gpu_tensor2.to(device=\"cpu\")\n",
    "\n",
    "# 使用简写的.cuda()或.cpu()也可转移张量\n",
    "gpu_tensor3 = gpu_tensor2.cuda(0)  # 默认索引也是0\n",
    "gpu_tensor4 = gpu_tensor3.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor和numpy之间的转换\n",
    "ten = torch.ones(2, 3)  # 在cpu上的张量\n",
    "\n",
    "# 转换为numpy类型，和张量存储共享相同的底层缓冲区，\n",
    "ten_np = ten.numpy()\n",
    "ten_np[0, 0] = 22  # 修改numpy类型，张量也会改变。\n",
    "\n",
    "# 将numpy类型转换为torch。# tensor和numpy之间的转换\n",
    "pytorch_tensor = torch.from_numpy(ten_np)\n",
    "ten = torch.ones(2, 3)  # 在cpu上的张量\n",
    "ten_np = ten.numpy()  # 转换为numpy类型，和张量存储共享相同的底层缓冲区，\n",
    "ten_np[0, 0] = 22  # 修改numpy类型，张量也会改变。\n",
    "pytorch_tensor = torch.from_numpy(ten_np)  # 将numpy类型转换为torch。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Dataset和DataLoader的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据为：\n",
      "\n",
      "tensor([[1., 2.]], device='cuda:0') tensor([[3.]], device='cuda:0')\n",
      "tensor([[3., 4.]], device='cuda:0') tensor([[7.]], device='cuda:0')\n",
      "tensor([[5., 6.]], device='cuda:0') tensor([[11.]], device='cuda:0')\n",
      "tensor([[7., 8.]], device='cuda:0') tensor([[15.]], device='cuda:0')\n",
      "损失值为：\n",
      " tensor(123.2943, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\世兰丁\\AppData\\Local\\Temp\\ipykernel_19156\\2661343386.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(x).float()\n",
      "C:\\Users\\世兰丁\\AppData\\Local\\Temp\\ipykernel_19156\\2661343386.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y).float()\n"
     ]
    }
   ],
   "source": [
    "# Dataset定义__len__和__getitem__方法，分别获取数据长度和每一份数据的信息，\n",
    "# DataLoader则成批次加载数据，\n",
    "# 传入的参数包括数据集Dataset、批次大小batch_size、是否打乱数据shuffle。\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# 简单的测试数据\n",
    "x = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
    "y = [[3], [7], [11], [15]]\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "X = torch.tensor(x).float()\n",
    "Y = torch.tensor(y).float()\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "\n",
    "\n",
    "# 创建自己的数据集类my_dataloader,继承自Dataset，重写__len__()和__getitem__()。\n",
    "class my_dataloader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.x[ix], self.y[ix]\n",
    "\n",
    "\n",
    "my_data = my_dataloader(X, Y)  # 获取所有的数据\n",
    "data_loader = DataLoader(my_data, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"测试数据为：\\n\")\n",
    "for x, y in data_loader:\n",
    "    print(x, y)\n",
    "\n",
    "\n",
    "class my_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(in_features=2, out_features=8)\n",
    "        self.hidden_active = nn.ReLU()\n",
    "        self.output_linear = nn.Linear(in_features=8, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_active(self.hidden_linear(x))\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# my_net = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "net = my_net()\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "def my_mean_squared_loss(_y, y):\n",
    "    loss = (_y - y) ** 2\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "loss_val = loss_func(net(X), Y)\n",
    "print(\"损失值为：\\n\", loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 自动梯度autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd。pytorch自动求梯度的方法，其基于计算图（一种有向无环图）\n",
    "\n",
    "如表达式z=wx+b,可以写成两个表达式：y=wx和z=y+b。x、w、b为变量，是用户创建的不依赖于其它变量的变量，故又称为叶子节点。\n",
    "为计算各叶子节点的梯度，需要把对应的张量参数requires_grad属性设置为True，这样就可自动跟踪其历史记录。\n",
    "y、z是计算得到的变量，非叶子节点，z为根节点。\n",
    "其中的乘法和加法是算子，由这些变量和算子就构成一个完整的计算过程（或向前传播过程）。\n",
    "\n",
    "pytorch调用backward()方法，会计算各节点的梯度，这是一个反向传播过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数w、b、x的梯度分别为:tensor([2.]),tensor([1.]),None\n",
      "非叶子节点y,z的梯度分别为:None,None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\世兰丁\\AppData\\Local\\Temp\\ipykernel_19156\\351135770.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"非叶子节点y,z的梯度分别为:{},{}\".format(y.grad, z.grad))\n"
     ]
    }
   ],
   "source": [
    "# （1）标量的反向传播\n",
    "\n",
    "# 假设x、w、b都是标量，z=wx+b，对标量z调用backward()方法，无须对backward()传入参数。\n",
    "import torch\n",
    "\n",
    "x = torch.Tensor([2])  # requires_grad默认为False\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "y = torch.multiply(w, x)\n",
    "z = torch.add(y, b)\n",
    "# print(x.requires_grad, w.requires_grad, b.requires_grad)  # 结果为False True True\n",
    "# print(y.requires_grad, z.requires_grad)  # 因为和w，b有依赖关系，所以y，z的requires_grad均为True。\n",
    "\n",
    "# 查看各节点是否为叶子结点。w、x、b是叶子节点，y、z不是，z是根节点。\n",
    "# print(w.is_leaf, x.is_leaf, b.is_leaf, y.is_leaf, z.is_leaf)  # True True True False False\n",
    "\n",
    "# 查看叶子节点的grad_fn属性，w、x、b的grad_fn为None，\n",
    "# y、z的grad_fn为<MulBackward0 object at 0x000001F70E182D70> <AddBackward0 object at 0x000001F70E182DA0>\n",
    "# print(w.grad_fn, x.grad_fn, b.grad_fn, y.grad_fn, z.grad_fn)\n",
    "\n",
    "# 基于z张量进行梯度反向传播,执行backward之后计算图会自动清空，\n",
    "z.backward()\n",
    "# 如果需要多次使用backward，需要修改参数retain_graph为True，此时梯度是累加的\n",
    "# z.backward(retain_graph=True)\n",
    "# 查看叶子节点的梯度，x是叶子节点但它无须求导，故其梯度为None\n",
    "print(\"参数w、b、x的梯度分别为:{},{},{}\".format(w.grad, b.grad, x.grad))\n",
    "\n",
    "# 参数w、b、x的梯度分别为:tensor([2.]),tensor([1.]),None\n",
    "# 非叶子节点的梯度，执行backward之后，会自动清空\n",
    "print(\"非叶子节点y,z的梯度分别为:{},{}\".format(y.grad, z.grad))\n",
    "# 非叶子节点y,z的梯度分别为:None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）非标量的反向传播\n",
    "\n",
    "目标张量一般都是标量，比如损失值loss。\n",
    "标量使用backward()方法无须传参，那非标量如何反向传播呢？\n",
    "\n",
    "PyTorch不让张量对张量求导，只允许标量对张量求导，\n",
    "因此，如果目标张量对一个非标量调用backward()，则需要传入一个gradient参数，\n",
    "该参数也是张量，而且需要与调用backward()的张量形状相同。\n",
    "传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。\n",
    "\n",
    "举一个例子来说，假设目标值为loss=(y1,y2,…,ym)，传入的参数为v=(v1,v2,…,vm)，\n",
    "那么就可把对loss的求导，转换为对loss*(v.T)（v.T表示v的转置）标量的求导。\n",
    "即将原来loss对x求偏导得到的雅可比矩阵乘以张量v.T（v的转置），便可得到需要的梯度矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [2., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义叶子节点张量x，形状为1x2\n",
    "x = torch.tensor([[2, 3]], dtype=torch.float, requires_grad=True)\n",
    "J = torch.zeros(2, 2)  # 初始化Jacobian矩阵\n",
    "y = torch.zeros(1, 2)  # 初始化目标张量，形状为1x2\n",
    "\n",
    "# 定义y与x之间的映射关系：y1=x1**2+3*x2，y2=2*x1+x2**2\n",
    "y[0, 0] = x[0, 0] ** 2 + 3 * x[0, 1]\n",
    "y[0, 1] = x[0, 1] ** 2 + 2 * x[0, 0]\n",
    "\n",
    "# y对x的梯度是一个雅可比矩阵，即[[2*x1,3],[2,2*x2]]。\n",
    "# 当x1=2,x2=3时，雅可比矩阵为[[4,3],[2,6]],其转置就为[[4,2],[3,6]]\n",
    "\n",
    "# 此时调用backward()获取y对x的梯度\n",
    "# y.backward(torch.Tensor([[1, 1]]))\n",
    "# print(x.grad)  # 结果为tensor([[6., 9.]])，这个结果和手工计算的结果不一致。\n",
    "\n",
    "# 错误的原因是v的取值，这样得到的并不是y对x的梯度，这里我们可以分成两步计算：\n",
    "# 首先让v=(1,0)得到y1对x的梯度，然后使v=(0,1)，得到y2对x的梯度。\n",
    "# 这里因需要重复使用backward()，需要使参数retain_graph=True，\n",
    "\n",
    "# 生成y1对x的梯度\n",
    "y.backward(torch.Tensor([[1, 0]]), retain_graph=True)\n",
    "J[0] = x.grad\n",
    "# 梯度是累加的，故需要对x的梯度清零。\n",
    "x.grad = torch.zeros_like(x.grad)\n",
    "\n",
    "# 生成y2对x的梯度\n",
    "y.backward(torch.Tensor([[0, 1]]))\n",
    "J[1] = x.grad\n",
    "\n",
    "# 显示jacobian矩阵的值\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）神经网络工具箱torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络层可以基于Module类或函数（nn.functional）。\n",
    "nn中的大多数层在functional中都有与之对应的函数。\n",
    "\n",
    "主要区别是nn.Module可自动提取可学习的参数，\n",
    "一般用在卷积层、全连接层、Dropout层等含有可学习参数的层。\n",
    "nn.functional则像是纯函数，一般用在激活函数、池化等无可学习参数的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处代码无法直接运行。需要下载好MNIST数据集，并修改训练、测试数据的路径。\n",
    "\n",
    "# 手写数字识别程序代码：\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import mnist  # 手写数字识别数据集\n",
    "import torchvision.transforms as transforms  # 变换\n",
    "from torch.utils.data import DataLoader  # 数据加载器\n",
    "import torch.nn.functional as F  # 函数functional\n",
    "import torch.optim as optim  # 优化器\n",
    "from torch import nn  # 神经网络工具箱\n",
    "\n",
    "# 定义超参数\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epoches = 20\n",
    "momentum = 0.5\n",
    "lr = 0.1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize([0.5], [0.5])  # 归一化，参数分别为均值和方差\n",
    "])\n",
    "# 加载数据\n",
    "train_dataset = mnist.MNIST('./data', train=True, transform=transform, download=False)\n",
    "test_dataset = mnist.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "import matplotlib.pyplot as plt  # 绘图工具\n",
    "\n",
    "# 源数据示例\n",
    "examples = enumerate(test_loader)  # 利用迭代器加载测试集的图片\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout() # 布局紧致，使子图填满整张图。\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# 定义神经网络类\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.BatchNorm1d(n_hidden_1))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.BatchNorm1d(n_hidden_2))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))\n",
    "\n",
    "    def forward(self, x):  # 前向传播\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 有GPU用GPU\n",
    "model = Net(28 * 28, 300, 100, 10)  # 实例化网络\n",
    "model.to(device)  # 模型移动到使用的设备上，此电脑是GPU。\n",
    "criterion = nn.CrossEntropyLoss()  # 损失函数\n",
    "optimizer = optim.SGD(model.parameters(), momentum=momentum)  # 优化器\n",
    "\n",
    "# 存储训练、测试数据的列表\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "for epoch in range(num_epoches):  # 训练\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()  # 切换到训练模式\n",
    "    \n",
    "    if epoch % 5 == 0:  # 每训练5轮学习率降为原来的0.1。\n",
    "        optimizer.param_groups[0]['lr'] *= lr\n",
    "        \n",
    "    for img, label in train_loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        train_acc += acc\n",
    "        \n",
    "    losses.append(train_loss / len(train_loader))\n",
    "    acces.append(train_acc / len(train_loader))\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for img, label in test_loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.item()\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        eval_acc += acc\n",
    "        \n",
    "    eval_losses.append(eval_loss / len(test_loader))\n",
    "    eval_acces.append(eval_acc / len(test_loader))\n",
    "    print('epoch:{},train_loss:{},train_acc:{},test_loss:{},test_acc:{}'\n",
    "          .format(epoch, train_loss / len(train_loader), train_acc / len(train_loader),\n",
    "                  eval_loss / len(test_loader), eval_acc / len(test_loader)))\n",
    "\n",
    "plt.title('trainloss')\n",
    "plt.plot(np.arange(len(losses)), losses)\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
